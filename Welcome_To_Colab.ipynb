{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2003UJAN/AI-Powered-Seat-Allocation-Optimization/blob/main/Welcome_To_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym torch numpy stable-baselines3\n"
      ],
      "metadata": {
        "id": "6YeNQ_wsvSXA",
        "outputId": "eff2525a-c1a9-4a32-8f0e-e42a4e4b08ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: stable-baselines3 in /usr/local/lib/python3.11/dist-packages (2.5.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym) (3.1.1)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (1.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (3.10.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3) (0.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (4.55.7)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "from gym import spaces"
      ],
      "metadata": {
        "id": "7ftIMbCYvXf7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SeatAllocationEnv(gym.Env):\n",
        "    def __init__(self, rows=10, cols=6, max_passengers=60):\n",
        "        super(SeatAllocationEnv, self).__init__()\n",
        "\n",
        "        self.rows = rows\n",
        "        self.cols = cols\n",
        "        self.max_passengers = max_passengers\n",
        "\n",
        "        self.seats = np.zeros((self.rows, self.cols))\n",
        "\n",
        "        self.passenger_prefs = [\"window\", \"aisle\", \"extra_legroom\"]\n",
        "\n",
        "        self.action_space = spaces.Discrete(self.rows * self.cols)\n",
        "        self.observation_space = spaces.Box(low=0, high=1, shape=(self.rows, self.cols), dtype=np.float32)\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the environment at the start of an episode.\"\"\"\n",
        "        self.seats = np.zeros((self.rows, self.cols))\n",
        "        return self.seats.flatten()\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Perform an action in the environment.\"\"\"\n",
        "        row, col = divmod(action, self.cols)\n",
        "\n",
        "        if self.seats[row, col] == 1:\n",
        "            reward = -5\n",
        "            done = False\n",
        "        else:\n",
        "            self.seats[row, col] = 1\n",
        "            reward = self._calculate_reward(row, col)\n",
        "            done = np.sum(self.seats) >= self.max_passengers\n",
        "\n",
        "        return self.seats.flatten(), reward, done, {}\n",
        "\n",
        "    def _calculate_reward(self, row, col):\n",
        "        reward = 1\n",
        "\n",
        "        if col == 0 or col == self.cols - 1:\n",
        "            reward += 2\n",
        "        elif col in [1, self.cols - 2]:\n",
        "            reward += 1\n",
        "\n",
        "        return reward\n",
        "\n",
        "    def render(self, mode=\"human\"):\n",
        "        print(self.seats)"
      ],
      "metadata": {
        "id": "YPqxAMHivboY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque"
      ],
      "metadata": {
        "id": "moyijzJ2waQC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)\n",
        "\n",
        "class DQNAgent:\n",
        "    def __init__(self, state_dim, action_dim, lr=0.001, gamma=0.99):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "\n",
        "        self.model = DQN(state_dim, action_dim)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "        self.loss_fn = nn.MSELoss()\n",
        "        self.memory = deque(maxlen=1000)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return np.random.randint(self.action_dim)\n",
        "        state = torch.FloatTensor(state).unsqueeze(0)\n",
        "        return torch.argmax(self.model(state)).item()\n",
        "\n",
        "    def store_experience(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def train(self, batch_size=32):\n",
        "        if len(self.memory) < batch_size:\n",
        "            return\n",
        "\n",
        "        batch = random.sample(self.memory, batch_size)\n",
        "        states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "        states = torch.FloatTensor(states)\n",
        "        actions = torch.LongTensor(actions)\n",
        "        rewards = torch.FloatTensor(rewards)\n",
        "        next_states = torch.FloatTensor(next_states)\n",
        "        dones = torch.FloatTensor(dones)\n",
        "\n",
        "        q_values = self.model(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
        "        next_q_values = self.model(next_states).max(1)[0].detach()\n",
        "        expected_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n",
        "\n",
        "        loss = self.loss_fn(q_values, expected_q_values)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay"
      ],
      "metadata": {
        "id": "VGd3uKgLwgvd"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env = SeatAllocationEnv()\n",
        "agent = DQNAgent(env.observation_space.shape[0] * env.observation_space.shape[1], env.action_space.n)\n",
        "\n",
        "num_episodes = 1000\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        action = agent.select_action(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        agent.store_experience(state, action, reward, next_state, done)\n",
        "        agent.train()\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "    print(f\"Episode {episode+1}, Total Reward: {total_reward}\")\n",
        "\n",
        "env.render()"
      ],
      "metadata": {
        "id": "1phCvre_wkzO",
        "outputId": "32dae339-8953-4308-c4d2-aa4b6cd4b688",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-fb6add2813f8>:48: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  dones = torch.FloatTensor(dones)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1, Total Reward: -41180\n",
            "Episode 2, Total Reward: -18540\n",
            "Episode 3, Total Reward: -13725\n",
            "Episode 4, Total Reward: -12445\n",
            "Episode 5, Total Reward: -12100\n",
            "Episode 6, Total Reward: -11695\n",
            "Episode 7, Total Reward: -48050\n",
            "Episode 8, Total Reward: -5655\n",
            "Episode 9, Total Reward: -22545\n",
            "Episode 10, Total Reward: -28190\n",
            "Episode 11, Total Reward: -5020\n",
            "Episode 12, Total Reward: -59935\n",
            "Episode 13, Total Reward: -19495\n",
            "Episode 14, Total Reward: -54960\n",
            "Episode 15, Total Reward: -144320\n",
            "Episode 16, Total Reward: -71000\n",
            "Episode 17, Total Reward: -58855\n",
            "Episode 18, Total Reward: -95150\n",
            "Episode 19, Total Reward: -29500\n",
            "Episode 20, Total Reward: -35485\n",
            "Episode 21, Total Reward: -123510\n",
            "Episode 22, Total Reward: -128310\n",
            "Episode 23, Total Reward: -87035\n",
            "Episode 24, Total Reward: -93810\n",
            "Episode 25, Total Reward: -91765\n",
            "Episode 26, Total Reward: -69515\n",
            "Episode 27, Total Reward: -90930\n",
            "Episode 28, Total Reward: -75855\n",
            "Episode 29, Total Reward: -77645\n",
            "Episode 30, Total Reward: -100375\n",
            "Episode 31, Total Reward: -91835\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO"
      ],
      "metadata": {
        "id": "tvnzWW1jwqFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ppo_model = PPO(\"MlpPolicy\", env, verbose=1)\n",
        "ppo_model.learn(total_timesteps=10000)\n",
        "\n",
        "obs = env.reset()\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    action, _ = ppo_model.predict(obs)\n",
        "    obs, reward, done, _ = env.step(action)\n",
        "    env.render()"
      ],
      "metadata": {
        "id": "wTi212H_wsOr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}